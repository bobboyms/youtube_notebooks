{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì **Import√¢ncia da GPU no treinamento de redes neurais**\n",
    "- üöÄ O uso de uma GPU pode acelerar o treinamento de uma rede neural em at√© 10 vezes\n",
    "- üß† As redes neurais s√£o compostas de uma rede de neur√¥nios artificiais, cada um dos quais executa uma opera√ß√£o matem√°tica simples. O treinamento de uma rede neural envolve ajustar os pesos desses neur√¥nios para que a rede possa aprender a realizar uma tarefa espec√≠fica.\n",
    "\n",
    "## üõ† **Entendendo as GPU**\n",
    "- üéÆ Originalmente, as GPUs foram projetadas para acelerar a renderiza√ß√£o gr√°fica em jogos e aplica√ß√µes visuais.\n",
    "- üìà Por volta de 2007/2008, as capacidades das GPUs foram estendidas para computa√ß√£o cient√≠fica, gra√ßas ao CUDA (Compute Unified Device Architecture)\n",
    "- üìä As GPUs s√£o usadas junto com uma CPU para agilizar o trabalho de opera√ß√µes numericamente intensivas.\n",
    "\n",
    "### ü§î **GPU vs CPU**\n",
    "\n",
    "#### : üñ• CPU:\n",
    "- Execu√ß√£o serial (executa opera√ß√µes uma ap√≥s a outra)\n",
    "- CPU carrega valores da mem√≥ria, realiza um c√°lculo com base nos valores e armazena o resultado de cada c√°lculo na mem√≥ria.\n",
    "- O acesso √† mem√≥ria √© lento em compara√ß√£o com a velocidade de c√°lculo e pode limitar a capacidade total das CPUs.\n",
    "- Possui no m√°ximo dezenas de CORES.\n",
    "\n",
    "#### üéÆ GPU\n",
    "- Execu√ß√£o paralela, pode executar milhares de multiplica√ß√µes e adi√ß√µes simultaneamente\n",
    "- GPU moderna geralmente milhares de CORES (ALUs - Unidades l√≥gicas aritm√©ticas)\n",
    "\n",
    "![](https://researchcomputing.princeton.edu/sites/g/files/toruqf311/files/styles/freeform_750w/public/2021-11/gpu_as_accelerator_to_cpu_diagram.png?itok=q0YaEuYH)\n",
    "\n",
    "A rela√ß√£o entre GPU e CPU:\n",
    "\n",
    "1. **Transfer√™ncia de Dados:** O CPU envia dados da mem√≥ria RAM para a GPU processar.\n",
    "2. **Instru√ß√µes:** O CPU dita √† GPU como os dados devem ser processados.\n",
    "3. **Administra√ß√£o de Mem√≥ria:** A GPU tem sua pr√≥pria mem√≥ria (VRAM), mas o CPU coordena o envio de dados para ela.\n",
    "4. **Coordena√ß√£o de Recursos:** O CPU organiza os recursos do sistema, garantindo que a GPU opere corretamente juntamente com outros componentes.\n",
    "\n",
    "### üî¢ **N√∫mero de n√∫cleos *GPU vs CPU***\n",
    "- üéÆ **GPU**: GeForce RTX 4090 (16,384 CUDA cores and 24GB of memory)\n",
    "- üñ• **CPU:**: AMD Ryzen Threadripper Pro 5995WX (64 cores)\n",
    "\n",
    "### üìù **Considera√ß√£o importante**\n",
    "\n",
    "üìå A efici√™ncia de uma GPU √© melhor aproveitada quando ela tem um grande volume de dados para processar. Isso ocorre porque uma GPU √© projetada para realizar muitas opera√ß√µes simples e paralelas ao mesmo tempo. Se a tarefa √© muito pequena ou simples, ent√£o a GPU pode n√£o ter oportunidade de usar todos os seus n√∫cleos de forma eficiente.\n",
    "\n",
    "üìå Al√©m disso, se a quantidade de dados for pequena, o tempo gasto para transferir esses dados da CPU para a GPU e vice-versa pode ser mais significativo em compara√ß√£o com o tempo realmente gasto no processamento, tornando a opera√ß√£o menos eficiente do que se tivesse sido realizada apenas na CPU.\n",
    "\n",
    "üìå Portanto, para tarefas com grandes volumes de dados que podem ser processados em paralelo, as GPUs s√£o geralmente muito mais r√°pidas do que as CPUs. Para tarefas menores e menos paraleliz√°veis, a CPU pode ser mais eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exemplo pr√°tico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "else:\n",
    "    raise Exception(\"GPU not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opera√ß√£o com matrix grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execu√ß√£o: 33.03488802909851 segundos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.rand(7000,3000, device=mps_device)\n",
    "    y = torch.rand(7000,3000, device=mps_device)\n",
    "\n",
    "    r = x @ y.T\n",
    "    r.sum()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tempo de execu√ß√£o: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execu√ß√£o: 53.21458578109741 segundos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.rand(7000,3000)\n",
    "    y = torch.rand(7000,3000)\n",
    "\n",
    "    r = x @ y.T\n",
    "    r.sum()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tempo de execu√ß√£o: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opera√ß√£o com tensores pequenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execu√ß√£o: 0.09651398658752441 segundos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.rand(70,30, device=mps_device)\n",
    "    y = torch.rand(70,30, device=mps_device)\n",
    "\n",
    "    r = x @ y.T\n",
    "    r.sum()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tempo de execu√ß√£o: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execu√ß√£o: 0.00587916374206543 segundos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.rand(70,30)\n",
    "    y = torch.rand(70,30)\n",
    "\n",
    "    r = x @ y.T\n",
    "    r.sum()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tempo de execu√ß√£o: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö **Bibliotecas de desenvolvimento para GPU**\n",
    "- **CUDA** (NVIDEA)\n",
    "- **Metal** (Apple)\n",
    "- **ROCm** (AMD)\n",
    "\n",
    "### **Unidade de processamento de tensores - TPU**\n",
    "- TPUs como um processador de matriz especializado em cargas de trabalho de redes neurais.\n",
    "- Em vez de fazer c√°lculos simples, como somar ou subtrair, ela √© projetada para fazer muitas multiplica√ß√µes de uma vez.\n",
    "- Elas podem processar opera√ß√µes de matriz maci√ßa, usadas em redes neurais em velocidades r√°pidas.\n",
    "- O ecossistema √© mais limitado, embora TensorFlow e alguns outros frameworks suportem TPUs.\n",
    "## **Quando escolher entre CPU, GPU e TPU**\n",
    "\n",
    "A escolha entre CPU, GPU e TPU para treinar uma rede neural depende de v√°rios fatores, incluindo o tamanho e a complexidade do modelo, a quantidade de dados, o or√ßamento, o tempo dispon√≠vel e as especificidades da tarefa. Vamos discutir os cen√°rios em que cada um deles seria apropriado:\n",
    "\n",
    "- **CPU:** Para modelos pequenos, prototipagem r√°pida, ou quando voc√™ est√° apenas come√ßando\n",
    "- **GPU:** Para treinamento de redes neurais de m√©dio a grande porte.\n",
    "- **TPU:** Para treinamento de redes neurais de grande escala, como modelos de linguagem com bilh√µes de par√¢metros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer√™ncias\n",
    "\n",
    "- https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n",
    "- https://cloud.google.com/tpu/docs/intro-to-tpu?hl=pt-br"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
