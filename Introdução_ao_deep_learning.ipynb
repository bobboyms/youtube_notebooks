{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbD7-sAcVRpL"
      },
      "source": [
        "# **Redes neurais artificiais**\n",
        "\n",
        "## **Hist√≥ria**\n",
        "  - üí° Primeiras ideias surgiram em 1943, proposta por McCulloch & Pitts (MCP)\n",
        "  - üí° Primeiro neur√¥nio artificial (*perceptron*) surgiu em 1958 por Frank Rosenblatt\n",
        "  - üí° O **backpropagation** (t√©cnica de treinamento das redes neurais) foi introduzido em 1974 por Paul Werbos\n",
        "  - üí° Em 1980, o \"**neocogitron**\" foi apresentado, precursor das modernas redes neurais convolucionais.\n",
        "  - üí° E em 1986, as Redes Neurais Recorrentes (RNNs) foram propostas.\n",
        "\n",
        "Grande parte dos fundamentos da intelig√™ncia artificial que vemos hoje tem ra√≠zes em pesquisas desenvolvidas d√©cadas atr√°s. No entanto, naquela √©poca, a combina√ß√£o de dados limitados e capacidade computacional restringia a experimenta√ß√£o em larga escala e a ado√ß√£o comercial da IA.\n",
        "\n",
        "## **Redes Neurais Artificiais vs. C√©rebros Humanos**\n",
        "\n",
        "- üß† **C√©rebros:** Bilh√µes de neur√¥nios interconectados.\n",
        "- üëå **Fun√ß√£o:** Perceber e reagir ao mundo.\n",
        "- üìî **ANNs:** Imitam sistemas neurais biol√≥gicos.\n",
        "- üí™ **Capacidade:** Aprendem eficientemente de dados experimentais.\n",
        "\n",
        "Em nossa biologia, os neur√¥nios s√£o c√©lulas especializadas que transmitem informa√ß√µes por meio de impulsos el√©tricos. No contexto das redes neurais artificiais, **essa transmiss√£o √© replicada atrav√©s de c√°lculos matem√°ticos**. O perceptron √© uma representa√ß√£o desse neur√¥nio no ambiente digital. A unidade fundamental de um modelo de rede neural √© o neur√¥nio, e, similarmente, a unidade fundamental de um modelo de rede neural artificial √© o perceptron.\n",
        "\n",
        "![img](https://d2f0ora2gkri0g.cloudfront.net/dd/db/dddb807b-a15b-457d-a21a-8a9e6f029a3e.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqfk0AzSf8SF"
      },
      "source": [
        "## **Principal matem√°tica do perceptron** (Algebra linear)\n",
        "\n",
        "üìâ √Ålgebra linear √© um ramo da matem√°tica que estuda vetores, espa√ßos vetoriais e transforma√ß√µes lineares entre esses espa√ßos, como rota√ß√µes e escalonamentos.\n",
        "\n",
        "### Produto escalar (dot product)\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} \\\n",
        "B = \\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix} \\\n",
        "$$\n",
        "\n",
        "$$\n",
        "A \\cdot B = \\sum_{i=1}^{4} A_i \\times B_i \\\n",
        "$$\n",
        "\n",
        "$$\n",
        "A \\cdot B = 1 \\times 5 + 2 \\times 6 + 3 \\times 7 + 4 \\times 8 = 70\n",
        "$$\n",
        "\n",
        "### Transposi√ß√£o de matriz\n",
        "\n",
        "![texto do link](https://2.bp.blogspot.com/-8LzbJv0zB3A/WAzRQbxP5eI/AAAAAAAAHYU/MEqPV8JxtLMCSGSQ-0UKZSYlUN3jALZaQCLcB/w589-h258/Java%2BProgram%2Bto%2BTranspose%2Ba%2BMatrix%2B.png)\n",
        "\n",
        "\n",
        "### Produto escalar em matriz (dot product)\n",
        "\n",
        "![texto do link](https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiUkqeSng7jl",
        "outputId": "b0dfe830-6959-4f0c-a6be-2325c9470a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n"
          ]
        }
      ],
      "source": [
        "A = [1, 2, 3, 4]\n",
        "B = [5, 6, 7, 8]\n",
        "\n",
        "produto_escalar = 0\n",
        "for i in range(len(A)):\n",
        "    produto_escalar += A[i] * B[i]\n",
        "\n",
        "print(produto_escalar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9kxnlx0lZvr"
      },
      "source": [
        "## **Bias**\n",
        "\n",
        "O bias oferece flexibilidade adicional ao modelo, garantindo que mesmo quando todas as entradas s√£o nulas, ainda possa haver uma sa√≠da devido √† sua influ√™ncia.\n",
        "\n",
        "O valor o bias tambem pode ser ajustado durante o treinamento, fazendo com que o modelo aprenda melhor. O bias oferece um grau adicional de liberdade ao modelo, permitindo que a fun√ß√£o aprendida se ajuste melhor aos dados. Sem um termo de bias, a fun√ß√£o estaria restrita a passar pela origem (no caso de modelos lineares, por exemplo),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18W0JCsZmSBT",
        "outputId": "0ac2e790-3e70-408d-a565-57425d59677d"
      },
      "outputs": [],
      "source": [
        "A = [1, 2, 3, 4]\n",
        "B = [0, 0, 0, 0]\n",
        "\n",
        "bias = 0.5\n",
        "\n",
        "produto_escalar = 0\n",
        "for i in range(len(A)):\n",
        "    produto_escalar += A[i] * B[i]\n",
        "\n",
        "print(\"Produto escalar:\", produto_escalar)\n",
        "print(\"Produto escalar + bias:\", produto_escalar + bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQA77pdADgcA"
      },
      "source": [
        "## **Equa√ß√£o que representa o neur√¥nio artificial**\n",
        "\n",
        "$$\n",
        "y = \\sum_{i=1}^{N} x_i w_i + b \\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzKqcsb-LmDE"
      },
      "source": [
        "## **Implementa√ß√£o do Neur√¥nio artificial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-CdBVIFKDj2",
        "outputId": "0cfae292-a2f4-47e2-94a9-b81cefda680c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Entradas\n",
        "X = tf.constant([1.,2.,3.])\n",
        "\n",
        "# Valor esperado de saida\n",
        "P = tf.Variable(42.)\n",
        "\n",
        "# Pesos e bias\n",
        "W = tf.Variable([6.,12.,7.])\n",
        "b = tf.Variable(9.)\n",
        "\n",
        "# Executa o calculo\n",
        "y = tf.reduce_sum(X * W) + b\n",
        "\n",
        "#Calculo do erro\n",
        "error = abs(P - y) / P\n",
        "\"Y\", y.numpy(), \"P\", P.numpy(), \"Error:\", error.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIiXxLFUGfY"
      },
      "source": [
        "## **Deep Learning** (ou redes com varias camadas)\n",
        "\n",
        "![texto do link](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png)\n",
        "\n",
        "## Equa√ß√£o que representa o neuronio que pode processar varias entradas ao mesmo tempo\n",
        "$$\n",
        "y = \\sum_{i=1}^{N} x_i w_i^T + b \\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q0cd6Es0Ugt"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, inputs, neurons, activation=None) -> None:\n",
        "        self.activation = activation\n",
        "        self.w = tf.Variable(tf.random.normal((neurons,inputs)))\n",
        "        self.b = tf.Variable(tf.random.normal((1, neurons)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = tf.matmul(x, tf.transpose(self.w)) + self.b\n",
        "        if self.activation:\n",
        "            return self.activation(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRvJ1AsT-JQd"
      },
      "source": [
        "## **Fun√ß√µes de ativa√ß√µes**\n",
        "\n",
        "### Linearmente separaveis\n",
        "\n",
        "Existem v√°rios tipos de rela√ß√µes entre vari√°veis em um conjunto de dados. Uma dessas rela√ß√µes √© a linearidade. Por exemplo, considere um cen√°rio em que voc√™ colete dados sobre o tempo de servi√ßo e os sal√°rios dos funcion√°rios de uma empresa. Se voc√™ observar que o sal√°rio aumenta de forma constante com o tempo de servi√ßo ‚Äî digamos, um aumento de 500 reais no sal√°rio para cada ano adicional na empresa ‚Äî isso √© um exemplo de uma rela√ß√£o linear entre as duas vari√°veis.\n",
        "\n",
        "Neste caso, o sal√°rio pode ser previsto como uma combina√ß√£o linear do tempo de servi√ßo, seguindo uma f√≥rmula como:\n",
        "\n",
        "$$\n",
        "Salario = 500 * \\text{Anos¬†na¬†empresa} + \\text{Salario inicial}\n",
        "$$\n",
        "\n",
        "Se gerarmos um gr√°fico para fun√ß√£o, podemos observar que √© uma linha reta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "dSHJO3V--Ou9",
        "outputId": "004963ba-d6f7-4396-9129-07aeeacc50b0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/thiagoluizrodrigues/Desktop/libtorch/IntroducÃßaÃÉo_ao_deep_learning (1).ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thiagoluizrodrigues/Desktop/libtorch/Introduc%CC%A7a%CC%83o_ao_deep_learning%20%281%29.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thiagoluizrodrigues/Desktop/libtorch/Introduc%CC%A7a%CC%83o_ao_deep_learning%20%281%29.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thiagoluizrodrigues/Desktop/libtorch/Introduc%CC%A7a%CC%83o_ao_deep_learning%20%281%29.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Definindo a fun√ß√£o\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Definindo a fun√ß√£o\n",
        "def salary_calc(year):\n",
        "    return 500 * year + 1000\n",
        "\n",
        "# Gerando dados para o gr√°fico\n",
        "years = np.linspace(0, 10, 100)  # Gera 100 pontos de 0 a 10 anos\n",
        "salaries = salary_calc(years)  # Calcula os sal√°rios correspondentes\n",
        "\n",
        "# Criando o gr√°fico\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(years, salaries, label='Sal√°rio em fun√ß√£o dos anos')\n",
        "plt.xlabel('Anos na Empresa')\n",
        "plt.ylabel('Sal√°rio (R$)')\n",
        "plt.title('Rela√ß√£o entre Anos na Empresa e Sal√°rio')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjRZBTVKr4AP"
      },
      "source": [
        "No entanto, a realidade √© frequentemente mais complexa e os relacionamentos entre as vari√°veis podem n√£o ser lineares. Al√©m disso, muitos problemas complexos, como reconhecimento de imagens e linguagem natural, s√£o inerentemente n√£o-lineares. Uma rede neural composta apenas por transforma√ß√µes lineares √© incapaz de aprender essas rela√ß√µes n√£o-lineares, n√£o importa quantas camadas voc√™ adicione.\n",
        "\n",
        "‚õ≥ O perceptron √© uma equa√ß√£o linear:\n",
        "\n",
        "$$\n",
        "y = x * w + b\n",
        "$$\n",
        "\n",
        "### **Exemplos de fun√ß√µes de ativa√ß√µes**\n",
        "\n",
        "![texto do link](https://ambrapaliaidata.blob.core.windows.net/ai-storage/articles/Untitled_design_13.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAqdYAKwN9oC",
        "outputId": "e708ba22-70ed-4844-f060-c36c686c2152"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Fun√ß√£o ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Fun√ß√£o Sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Testando as fun√ß√µes\n",
        "x_values = np.array([-2, -1, 0, 1, 2])\n",
        "\n",
        "print(\"ReLU de x_values:\", relu(x_values))\n",
        "print(\"Sigmoid de x_values:\", sigmoid(x_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDCwNpKgOD6F"
      },
      "source": [
        "### N√£o linearmente separaveis\n",
        "\n",
        "**Tabela verdade operador XOR**\n",
        "\n",
        "| Input A | Input B | Output |\n",
        "|---------|---------|--------|\n",
        "|    0    |    0    |    0   |\n",
        "|    0    |    1    |    1   |\n",
        "|    1    |    0    |    1   |\n",
        "|    1    |    1    |    0   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "QXVxRngMxfg1",
        "outputId": "6eeb2d5b-b643-4ddb-9e29-38206512baed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Coordenadas para os pontos\n",
        "x_coords = [0, 0, 1, 1]\n",
        "y_coords = [0, 1, 0, 1]\n",
        "\n",
        "# R√≥tulos para os pontos (outputs da porta XOR)\n",
        "labels = [0, 1, 1, 0]\n",
        "\n",
        "# Cores para os pontos (azul para 0 e vermelho para 1)\n",
        "colors = ['blue' if label == 0 else 'red' for label in labels]\n",
        "\n",
        "# Criando o gr√°fico\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(x_coords, y_coords, c=colors)\n",
        "\n",
        "# Adicionando r√≥tulos aos pontos\n",
        "for i, (x, y) in enumerate(zip(x_coords, y_coords)):\n",
        "    plt.text(x + 0.05, y, str(labels[i]))\n",
        "\n",
        "# Configurando os eixos\n",
        "plt.xlim(-0.5, 1.5)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "plt.xlabel('Input A')\n",
        "plt.ylabel('Input B')\n",
        "plt.title('XOR Problem')\n",
        "\n",
        "# Mostrando o gr√°fico\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4fHJ66ZwBw1",
        "outputId": "8ae47416-992e-45f8-a157-f3319092661b"
      },
      "outputs": [],
      "source": [
        "# Dados de entrada e sa√≠da (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# Criando a rede neural com fun√ß√£o de ativa√ß√£o\n",
        "layer1 = Dense(inputs=2, neurons=7, activation=tf.nn.relu)\n",
        "layer2 = Dense(7, 3, activation=tf.nn.relu)\n",
        "layer3 = Dense(3, 1)\n",
        "\n",
        "# Criando a rede neural com fun√ß√£o de ativa√ß√£o\n",
        "# layer1 = Dense(2, 7)\n",
        "# layer2 = Dense(7, 3)\n",
        "# layer3 = Dense(3, 1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 150\n",
        "loss_fn = tf.losses.MeanSquaredError()\n",
        "\n",
        "# Treinamento\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([layer1.w, layer1.b, layer2.w, layer2.b])\n",
        "        output1 = layer1.forward(X)\n",
        "        output2 = layer2.forward(output1)\n",
        "        output3 = layer3.forward(output2)\n",
        "        loss = loss_fn(y, output3)\n",
        "\n",
        "    if tf.math.is_nan(loss):\n",
        "      print(\"Loss is NaN. Stopping training.\")\n",
        "      break\n",
        "\n",
        "    gradients = tape.gradient(loss, [layer1.w, layer1.b, layer2.w, layer2.b, layer3.w, layer3.b])\n",
        "\n",
        "    # Atualizar pesos e bias\n",
        "    layer1.w.assign_sub(learning_rate * gradients[0])\n",
        "    layer1.b.assign_sub(learning_rate * gradients[1])\n",
        "    layer2.w.assign_sub(learning_rate * gradients[2])\n",
        "    layer2.b.assign_sub(learning_rate * gradients[3])\n",
        "    layer3.w.assign_sub(learning_rate * gradients[4])\n",
        "    layer3.b.assign_sub(learning_rate * gradients[5])\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Fazer previs√µes\n",
        "output1 = layer1.forward(X)\n",
        "output2 = layer2.forward(output1)\n",
        "output3 = layer3.forward(output2)\n",
        "\n",
        "print(\"Previs√µes:\")\n",
        "print(np.round(output3.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaBuww9jxzfP",
        "outputId": "ac705c67-51ed-49c2-fbcd-598920c5d022"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input_tensor = torch.tensor([-3.0, -0.5, 0.0, 0.5, 1.0], requires_grad=True)\n",
        "output_tensor = torch.relu(input_tensor)\n",
        "\n",
        "print(output_tensor)\n",
        "\n",
        "c = output_tensor.sum()\n",
        "c.backward()\n",
        "\n",
        "input_tensor.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y26cb5pai2nZ"
      },
      "source": [
        "## **Aprendizado da rede neural**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ffjKwnSuw_"
      },
      "source": [
        "### Calculo diferencial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbIZ26kIlf5X",
        "outputId": "144e3822-fe46-4a8e-beb8-8fa552a0e794"
      },
      "outputs": [],
      "source": [
        "tempo = 11.75\n",
        "\n",
        "def calcula_distancia(tempo):\n",
        "  return tempo**2\n",
        "\n",
        "distancia = calcula_distancia(tempo)\n",
        "print(\"Distancia percorrida:\",distancia)\n",
        "\n",
        "def calcula_distancia_derivada(tempo):\n",
        "  return 2 * tempo\n",
        "\n",
        "velocidade = calcula_distancia_derivada(tempo)\n",
        "print(\"Velocidade instantanea:\",velocidade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yje5nikRSLs2"
      },
      "source": [
        "### Calculo a descida do gradiente\n",
        "\n",
        "üèé Para atingir uma distancia de 12.354, qual √© o tempo que eu preciso?\n",
        "\n",
        "### Gradiente\n",
        "\n",
        "üßë \"Um gradiente mede o quanto a sa√≠da de uma fun√ß√£o muda se voc√™ alterar um pouco as entradas.\" - Lex Fridman (MIT)\n",
        "\n",
        "\n",
        "‚Üò O Gradiente da fun√ß√£o de custo ÔøΩ(ÔøΩ(ÔøΩ))J(f(x)) indica tanto a dire√ß√£o quanto a \"for√ßa\" ou magnitude do ajuste que deve ser feito nos par√¢metros (pesos e vieses) para minimizar a fun√ß√£o de custo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EB0jFI118-F",
        "outputId": "30c95899-bf41-4c6c-823d-e70b7b66c750"
      },
      "outputs": [],
      "source": [
        "# Valor alvo\n",
        "distancia_alvo = 12.256\n",
        "\n",
        "# Valor inicial de tempo\n",
        "tempo = 9.658  # Voc√™ pode come√ßar com qualquer valor\n",
        "\n",
        "# Taxa de aprendizado (pa√ßo)\n",
        "learning_rate = 0.001\n",
        "\n",
        "# N√∫mero de itera√ß√µes\n",
        "n_iteracoes = 100\n",
        "\n",
        "def calc_erro(valor_atual, valor_alvo):\n",
        "  return (valor_atual - valor_alvo)**2\n",
        "\n",
        "def calc_erro_derivado(valor_atual, valor_alvo):\n",
        "  return 2 * (valor_atual - valor_alvo)\n",
        "\n",
        "valores_erro = []\n",
        "valores_x = []\n",
        "for i in range(n_iteracoes):\n",
        "    # Calcula o valor atual da fun√ß√£o\n",
        "    try:\n",
        "        distancia_percorrida = calcula_distancia(tempo)\n",
        "    except OverflowError:\n",
        "        break\n",
        "\n",
        "    # Calcula o erro atual\n",
        "    erro = calc_erro(distancia_percorrida, distancia_alvo)\n",
        "    valores_erro.append(erro)\n",
        "\n",
        "    # Inicia o backpropagation\n",
        "    # Calcula o gradiente (derivada) da fun√ß√£o no ponto atual\n",
        "    grad_x = calcula_distancia_derivada(tempo)\n",
        "    grad_y = calc_erro_derivado(distancia_percorrida, distancia_alvo)\n",
        "\n",
        "    # Obtem a dire√ß√£o e a for√ßa do ajuste\n",
        "    direcao_ajuste_e_forca = grad_x * grad_y\n",
        "\n",
        "    # Atualiza o valor de tempo usando gradiente descendente\n",
        "    tempo = tempo - learning_rate * direcao_ajuste_e_forca\n",
        "    valores_x.append(distancia_percorrida)\n",
        "\n",
        "    print(f\"Itera√ß√£o {i+1}: Erro = {erro} ,Direcao ajuste e for√ßa = {direcao_ajuste_e_forca}\")\n",
        "\n",
        "# Valor final de x\n",
        "print(f\"Valor final de Tempo = {tempo}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_ERGMf55hku",
        "outputId": "8bea921f-ecdd-4e8a-e4a3-b75950b98388"
      },
      "outputs": [],
      "source": [
        "calcula_distancia_derivada(calcula_distancia(3.500898000504678))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "kzkrQxG9bF3o",
        "outputId": "c7d73f0b-45da-4fc8-c1cd-7e84d4cbb6d0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Criar o gr√°fico\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Erro ao longo das itera√ß√µes')\n",
        "plt.xlabel('Itera√ß√µes')\n",
        "plt.ylabel('Erro')\n",
        "plt.plot(valores_erro)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Valor de Tempo ao longo das itera√ß√µes')\n",
        "plt.xlabel('Itera√ß√µes')\n",
        "plt.ylabel('Tempo')\n",
        "plt.plot(valores_x)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiUUG3S7ebig"
      },
      "source": [
        "### Exemplo descida do gradiente\n",
        "\n",
        "![texto do link](https://duchesnay.github.io/pystatsml/_images/learning_rate_choice.png)\n",
        "\n",
        "### Descida do gradiente com perceptron\n",
        "\n",
        "![texto do link](https://test.basel.in/wp-content/uploads/2019/09/GRADIENT-DECENT.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wBkLxuZfMQ8"
      },
      "source": [
        "### Exemplo da descida do gradiente com TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsorn8Enk2rp"
      },
      "source": [
        "## **Exemplo pr√°tico**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un5CiP-hlxdB"
      },
      "source": [
        "### Carregando o dataset MNIST\n",
        "\n",
        "![texto do link](https://miro.medium.com/v2/resize:fit:720/format:webp/1*VOP5sC-T2EWm8RmBNGpCUg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JehlStkKl1I9",
        "outputId": "077cfff2-2441-4053-8d06-8b23d1c2afed"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Carregando o dataset MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalizando os dados\n",
        "x_train = x_train.reshape(-1, 28*28)  # Achatando as imagens\n",
        "x_test = x_test.reshape(-1, 28*28)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)  # Convertendo para one-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_test = np.array(x_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Qku-HQudusJ2",
        "outputId": "55816af9-fd00-4f09-d7c2-a4954753ce56"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizando as primeiras 10 imagens\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')  # Nota o uso de reshape aqui\n",
        "    plt.title(f\"Label: {np.argmax(y_test[i])}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oTArS3cmUOo"
      },
      "source": [
        "### Executando o treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "QeW_fhyqmWxO",
        "outputId": "d0acb557-7080-46e6-9b8a-32c09d22347c"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, inputs, neurons, activation=None):\n",
        "        self.activation = activation\n",
        "        self.w = tf.Variable(tf.random.normal((neurons, inputs), dtype=tf.float32))\n",
        "        self.b = tf.Variable(tf.random.normal((1, neurons), dtype=tf.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = tf.matmul(x, tf.transpose(self.w)) + self.b\n",
        "        if self.activation:\n",
        "            return self.activation(z)\n",
        "        return z\n",
        "\n",
        "# Par√¢metros\n",
        "learning_rate = 1\n",
        "epochs = 3500\n",
        "loss_fn = tf.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Construindo a rede neural\n",
        "layer1 = Dense(inputs=28*28, neurons=128, activation=tf.nn.relu)\n",
        "layer2 = Dense(128, 64, activation=tf.nn.relu)\n",
        "layer3 = Dense(64, 10, activation=tf.nn.softmax)\n",
        "\n",
        "# Treinamento\n",
        "for epoch in range(epochs+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([layer1.w, layer1.b, layer2.w, layer2.b, layer3.w, layer3.b])\n",
        "        output1 = layer1.forward(x_train)\n",
        "        output2 = layer2.forward(output1)\n",
        "        output3 = layer3.forward(output2)\n",
        "        loss = loss_fn(y_train, output3)\n",
        "\n",
        "    gradients = tape.gradient(loss, [layer1.w, layer1.b, layer2.w, layer2.b, layer3.w, layer3.b])\n",
        "\n",
        "    layer1.w.assign_sub(learning_rate * gradients[0])\n",
        "    layer1.b.assign_sub(learning_rate * gradients[1])\n",
        "    layer2.w.assign_sub(learning_rate * gradients[2])\n",
        "    layer2.b.assign_sub(learning_rate * gradients[3])\n",
        "    layer3.w.assign_sub(learning_rate * gradients[4])\n",
        "    layer3.b.assign_sub(learning_rate * gradients[5])\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Avalia√ß√£o\n",
        "output1 = layer1.forward(x_test)\n",
        "output2 = layer2.forward(output1)\n",
        "output3 = layer3.forward(output2)\n",
        "predictions = np.argmax(output3.numpy(), axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "accuracy = np.sum(predictions == true_labels) / len(true_labels) * 100\n",
        "print(f\"Acur√°cia: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNCXBigXwz0n"
      },
      "source": [
        "### Executando uma previs√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjd4nZCGwgfA"
      },
      "outputs": [],
      "source": [
        "plt.subplot(2, 5, i+1)\n",
        "plt.imshow(x_test[102].reshape(28, 28), cmap='gray')\n",
        "plt.title(f\"Label: {np.argmax(y_test[29])}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7hxFj3t6we"
      },
      "outputs": [],
      "source": [
        "input_data = np.expand_dims(x_test[32], axis=0)\n",
        "output1 = layer1.forward(input_data)\n",
        "output2 = layer2.forward(output1)\n",
        "output3 = layer3.forward(output2)\n",
        "predictions = np.argmax(output3.numpy(), axis=1)\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Q3FuemoIFD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Configura√ß√£o de semente para reprodutibilidade\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Carregar o dataset\n",
        "digits = datasets.load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Normaliza√ß√£o\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encoding dos r√≥tulos\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Dividir em treinamento e teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converte para floa32\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_test = np.array(x_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, inputs, neurons, activation=None):\n",
        "        self.activation = activation\n",
        "        self.w = tf.Variable(tf.random.normal((neurons, inputs), dtype=tf.float32))\n",
        "        self.b = tf.Variable(tf.random.normal((1, neurons), dtype=tf.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = tf.matmul(x, tf.transpose(self.w)) + self.b\n",
        "        if self.activation:\n",
        "            return self.activation(z)\n",
        "        return z\n",
        "\n",
        "# Par√¢metros\n",
        "learning_rate = 0.01\n",
        "epochs = 1200\n",
        "loss_fn = tf.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Construindo a rede neural\n",
        "layer1 = Dense(inputs=64, neurons=128, activation=tf.nn.relu)  # 64 features no dataset digits\n",
        "layer2 = Dense(128, 64, activation=tf.nn.relu)\n",
        "layer3 = Dense(64, 10, activation=None)  # 10 classes no dataset digits\n",
        "\n",
        "# Treinamento\n",
        "for epoch in range(epochs + 1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([layer1.w, layer1.b, layer2.w, layer2.b, layer3.w, layer3.b])\n",
        "        output1 = layer1.forward(x_train)\n",
        "        output2 = layer2.forward(output1)\n",
        "        output3 = layer3.forward(output2)\n",
        "        loss = loss_fn(y_train, output3)\n",
        "\n",
        "    gradients = tape.gradient(loss, [layer1.w, layer1.b, layer2.w, layer2.b, layer3.w, layer3.b])\n",
        "\n",
        "    layer1.w.assign_sub(learning_rate * gradients[0])\n",
        "    layer1.b.assign_sub(learning_rate * gradients[1])\n",
        "    layer2.w.assign_sub(learning_rate * gradients[2])\n",
        "    layer2.b.assign_sub(learning_rate * gradients[3])\n",
        "    layer3.w.assign_sub(learning_rate * gradients[4])\n",
        "    layer3.b.assign_sub(learning_rate * gradients[5])\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Avalia√ß√£o\n",
        "output1 = layer1.forward(x_test)\n",
        "output2 = layer2.forward(output1)\n",
        "output3 = layer3.forward(output2)\n",
        "predictions = np.argmax(output3.numpy(), axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "accuracy = np.sum(predictions == true_labels) / len(true_labels) * 100\n",
        "print(f\"Acur√°cia: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuB8YAmSP0uc"
      },
      "source": [
        "# Implementando um RNN\n",
        "\n",
        "- https://jmyao17.github.io/Machine_Learning/Sequence/RNN-1.html\n",
        "- https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini\n",
        "- https://www.youtube.com/watch?v=6niqTuYFZLQ&ab_channel=StanfordUniversitySchoolofEngineering\n",
        "- https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2&ab_channel=AlexanderAmini\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGvqaUs6P4QX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Configura√ß√£o de semente para reprodutibilidade\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, activation=tf.math.tanh):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        # Peso e bias (inicializados aleatoriamente)\n",
        "        self.W_aa = tf.Variable(tf.random.normal((hidden_dim, hidden_dim)))\n",
        "        self.W_ax = tf.Variable(tf.random.normal((hidden_dim, input_dim)))\n",
        "        self.W_ya = tf.Variable(tf.random.normal((output_dim, hidden_dim)))\n",
        "        self.b_a = tf.Variable(tf.random.normal((hidden_dim, 1)))\n",
        "        self.b_y = tf.Variable(tf.random.normal((output_dim, 1)))\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def step(self, a_prev, x_t):\n",
        "        a_next = self.activation(tf.linalg.matmul(self.W_aa, a_prev) + tf.linalg.matmul(self.W_ax, x_t) + self.b_a)\n",
        "        return a_next\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, x):\n",
        "        a_prev = tf.zeros((self.hidden_dim, 1))\n",
        "        states = tf.scan(self.step, elems=x, initializer=a_prev)\n",
        "        a_last = states[-1]\n",
        "        return tf.linalg.matmul(self.W_ya, a_last) + self.b_y\n",
        "\n",
        "input_dim = 1\n",
        "output_dim = 2\n",
        "hidden_dim = 5\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Inicializando a RNN e o otimizador\n",
        "rnn = RNN(input_dim, output_dim, hidden_dim)\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Dados de exemplo (substitua por seus pr√≥prios dados)\n",
        "x = tf.random.normal((10, 1, 1))  # 10 timesteps, dimens√£o de entrada 1\n",
        "y_true = tf.constant([[1.0], [0.0]])  # Valor verdadeiro (substitua pelo seu valor verdadeiro)\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = rnn.forward(x)\n",
        "        loss = tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error\n",
        "\n",
        "    grads = tape.gradient(loss, [rnn.W_aa, rnn.W_ax, rnn.W_ya, rnn.b_a, rnn.b_y])\n",
        "    optimizer.apply_gradients(zip(grads, [rnn.W_aa, rnn.W_ax, rnn.W_ya, rnn.b_a, rnn.b_y]))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfCRoOE8qfto"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Inicializar a e b como tensores com requires_grad=True\n",
        "a = torch.tensor([[1.2, 2.5], [3.2, 4.5], [3.5, 2.5]], requires_grad=True)\n",
        "b = torch.tensor([[5.2, 2.3], [3.5, 6.5], [8.5, 6.5]])\n",
        "\n",
        "# Definir a taxa de aprendizado\n",
        "lr = 0.01\n",
        "\n",
        "# Simular v√°rias etapas de treinamento\n",
        "for step in range(100):\n",
        "    # Calcule a perda\n",
        "    c = (a - b)**2\n",
        "\n",
        "    # Realize backpropagation para calcular os gradientes\n",
        "    loss = c.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    # Atualize os par√¢metros\n",
        "    with torch.no_grad():  # Desativar a grava√ß√£o de opera√ß√µes para a atualiza√ß√£o\n",
        "        a -= lr * a.grad\n",
        "\n",
        "    # Zerar os gradientes acumulados\n",
        "    a.grad.zero_()\n",
        "\n",
        "    # Imprimir a perda a cada 10 etapas\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}, Loss {loss.item()}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
